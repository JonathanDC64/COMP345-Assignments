\hypertarget{classword__tokenizer}{}\section{word\+\_\+tokenizer Class Reference}
\label{classword__tokenizer}\index{word\+\_\+tokenizer@{word\+\_\+tokenizer}}
Inheritance diagram for word\+\_\+tokenizer\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{classword__tokenizer}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classword__tokenizer_a30887805b70d364fb6fde6cdd814c16a}{word\+\_\+tokenizer} ()
\item 
\hyperlink{classword__tokenizer_a19a17890aac719ba8cb1459713f5a96e}{$\sim$word\+\_\+tokenizer} ()
\item 
virtual std\+::vector$<$ std\+::string $>$ \hyperlink{classword__tokenizer_ac51f61652880447073e16fdb0f471fbe}{tokenize} (const \hyperlink{classdocument}{document} \&filename) override
\end{DoxyCompactItemize}
\subsection*{Friends}
\begin{DoxyCompactItemize}
\item 
std\+::ostream \& \hyperlink{classword__tokenizer_a570983076ba46499d5bebccccac99a4a}{operator$<$$<$} (std\+::ostream \&os, const \hyperlink{classword__tokenizer}{word\+\_\+tokenizer} \&tk)
\end{DoxyCompactItemize}


\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classword__tokenizer_a30887805b70d364fb6fde6cdd814c16a}\label{classword__tokenizer_a30887805b70d364fb6fde6cdd814c16a}} 
\index{word\+\_\+tokenizer@{word\+\_\+tokenizer}!word\+\_\+tokenizer@{word\+\_\+tokenizer}}
\index{word\+\_\+tokenizer@{word\+\_\+tokenizer}!word\+\_\+tokenizer@{word\+\_\+tokenizer}}
\subsubsection{\texorpdfstring{word\+\_\+tokenizer()}{word\_tokenizer()}}
{\footnotesize\ttfamily word\+\_\+tokenizer\+::word\+\_\+tokenizer (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}

Default constructor \mbox{\Hypertarget{classword__tokenizer_a19a17890aac719ba8cb1459713f5a96e}\label{classword__tokenizer_a19a17890aac719ba8cb1459713f5a96e}} 
\index{word\+\_\+tokenizer@{word\+\_\+tokenizer}!````~word\+\_\+tokenizer@{$\sim$word\+\_\+tokenizer}}
\index{````~word\+\_\+tokenizer@{$\sim$word\+\_\+tokenizer}!word\+\_\+tokenizer@{word\+\_\+tokenizer}}
\subsubsection{\texorpdfstring{$\sim$word\+\_\+tokenizer()}{~word\_tokenizer()}}
{\footnotesize\ttfamily word\+\_\+tokenizer\+::$\sim$word\+\_\+tokenizer (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}

Destructor 

\subsection{Member Function Documentation}
\mbox{\Hypertarget{classword__tokenizer_ac51f61652880447073e16fdb0f471fbe}\label{classword__tokenizer_ac51f61652880447073e16fdb0f471fbe}} 
\index{word\+\_\+tokenizer@{word\+\_\+tokenizer}!tokenize@{tokenize}}
\index{tokenize@{tokenize}!word\+\_\+tokenizer@{word\+\_\+tokenizer}}
\subsubsection{\texorpdfstring{tokenize()}{tokenize()}}
{\footnotesize\ttfamily std\+::vector$<$ std\+::string $>$ word\+\_\+tokenizer\+::tokenize (\begin{DoxyParamCaption}\item[{const \hyperlink{classdocument}{document} \&}]{filename }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [override]}, {\ttfamily [virtual]}}

returns a vector containing all the words (sanitized) from a given file 

Implements \hyperlink{classabstract__tokenizer_aeea4a861fdfa18351555b0d00a378165}{abstract\+\_\+tokenizer$<$ std\+::string $>$}.



\subsection{Friends And Related Function Documentation}
\mbox{\Hypertarget{classword__tokenizer_a570983076ba46499d5bebccccac99a4a}\label{classword__tokenizer_a570983076ba46499d5bebccccac99a4a}} 
\index{word\+\_\+tokenizer@{word\+\_\+tokenizer}!operator$<$$<$@{operator$<$$<$}}
\index{operator$<$$<$@{operator$<$$<$}!word\+\_\+tokenizer@{word\+\_\+tokenizer}}
\subsubsection{\texorpdfstring{operator$<$$<$}{operator<<}}
{\footnotesize\ttfamily std\+::ostream\& operator$<$$<$ (\begin{DoxyParamCaption}\item[{std\+::ostream \&}]{os,  }\item[{const \hyperlink{classword__tokenizer}{word\+\_\+tokenizer} \&}]{tk }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [friend]}}

prints the the use of the tokenizer class 

The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
word\+\_\+tokenizer.\+h\item 
word\+\_\+tokenizer.\+cpp\end{DoxyCompactItemize}
